# -*- coding: utf-8 -*-
"""boston.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LRhzsiu62OESMPc2lHrBgGbhPaMmSQh6
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston

import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

boston = load_boston()

print(boston.keys())

print(boston.DESCR)

print(boston.feature_names)

bostData = pd.DataFrame(boston.data, columns=boston.feature_names)
bostData.head()

# Adding the target variable to the dataframe

bostData["MEDV"] = boston.target
bostData.head()

bostData.describe()

"""### **EDA using pandas profiling report**"""

#!pip install pandas_profiling==2.8.0

#import pandas_profiling
#report = pandas_profiling.ProfileReport(bostData)
#report.to_file("profilReport.html")

"""# **Finding the relationship between each features and label(target / dependent column)**"""

#import seaborn as sns

sns.pairplot(bostData,x_vars=boston.feature_names,y_vars="MEDV",kind="reg")
plt.show()

"""**Seems the above pairplot is not that much effective for analysis, so will give a try usiung heatmap, but already from the profiling report we can have a look of correlation**"""

bostCorr = bostData.corr()
#print(bostCorr)
plt.figure(figsize=(15,10))
sns.heatmap(bostCorr,linewidths = .4,cmap = 'coolwarm',fmt=".2f",cbar_kws = {'shrink': .2},vmin = -1, vmax = 1, annot = True)
plt.show()

"""From the above heatmap and from profiling report , we can see that there is high correlation between **RAD** & **TAX** columns, so need to take appropriate action.

From the above HeatMap, we can observe that there is high correlation(+ve and -ve) for LSTAT and RM with target MEDV

Will get the coefficients for each feature and can check the values.
"""

#Feature selection using Wrapper method

x_FCheck = bostData.drop("MEDV",axis=1)
y_FCheck = bostData[["MEDV"]]

import statsmodels.api as sm

XConst = sm.add_constant(x_FCheck)
model = sm.OLS(y_FCheck,XConst).fit()
print(model.pvalues)

"""Will exclude the AGE having high P-value & RAD as having high correlation with TAX with high P-value compared with TAX."""

x = bostData[["CRIM","ZN","CHAS","NOX","RM","DIS","PTRATIO","B","LSTAT"]]
y = bostData[["MEDV"]]

#from sklearn.model_selection import train_test_split
#from sklearn.linear_model import LinearRegression

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=42)

# define the model

LinearRegModel = LinearRegression()

# fit the model
LinearRegModel.fit(x_train,y_train)

# Saving model to disk
import pickle
pickle.dump(LinearRegModel, open('model.pkl','wb'))


# Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))
print(model.predict([[1,2,3,4,5,6,7,8,9]]))

#print(model.coef_)

#y_predict = model.predict(x_test)

#print(model.score(x_test,y_test))

# Accuracy metrix

#R2 & MSE

#from sklearn.metrics import mean_squared_error,r2_score
#print("R2 value:" , r2_score(y_test,y_predict))
#print("MSE vlaue:" , mean_squared_error(y_test,y_predict))